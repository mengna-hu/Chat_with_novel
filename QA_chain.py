from langchain_community.llms.sparkllm import SparkLLM
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate,ChatPromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel
from build_db import build,load_db
from embedding import get_embedding
from dotenv import load_dotenv,find_dotenv
import os
import re

_ = load_dotenv(find_dotenv())

def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def get_vectordb(embedding="è®¯é£æ˜Ÿç«", persisr_dir='./vector_db/chroma', file_path='./novels/',embedding_key=None):
    if os.path.exists(persisr_dir):
        content = os.listdir(persisr_dir)
        if len(content) == 0:
            vector_db = build(file_path, persisr_dir, embedding)
    else:
        vector_db = build(file_path, persisr_dir, embedding)
    vector_db = load_db(persisr_dir, embedding)

    return vector_db

def model_to_llm(model='generalv3.5', temperature=0.01):
    if model  == "Spark-max":
        llm = SparkLLM(
            model=model,
            app_id=os.environ["API_ID"],
            api_key=os.environ["API_Key"],
            api_secret=os.environ["API_Secret"],
            temperature=temperature,
            spark_api_url="wss://spark-api.xf-yun.com/v3.5/chat",
            spark_llm_domain = "generalv3.5"
        )
    elif model == "Spark-Ultra":
        llm = SparkLLM(
            model=model,
            app_id=os.environ["API_ID"],
            api_key=os.environ["API_Key"],
            api_secret=os.environ["API_Secret"],
            temperature=temperature,
            spark_api_url="wss://spark-api.xf-yun.com/v4.0/chat",
            spark_llm_domain = "4.0Ultra"
        )
    elif model == "Chatglm-4.5":
        llm = ChatOpenAI(
            model = "glm-4.5",
            temperature = temperature,
            openai_api_key = os.environ["ZHIPUAI_API_key"],
            openai_api_base = "https://open.bigmodel.cn/api/paas/v4/"
        )
    elif model == "Qwen-plus":
        llm = ChatOpenAI(
            model = "qwen-plus",
            api_key= os.environ["QIANWEN_API_Key"],
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
    elif model == "ernie-speed":
        llm = ChatOpenAI(
            model= "ernie-speed-128k",
            api_key= os.environ["QIANFAN_API_Key"],
            base_url="https://qianfan.baidubce.com/v2"
        )
    return llm

class Chat_QA_with_context:
    def __init__(self, model, temperature, top_k, chat_history, embedding, history_len, persist_dir='./vector_db/chroma', file_path='./novels/'):
        self.model = model
        self.temperature = temperature
        self.top_k = top_k
        self.chat_history = chat_history
        self.embedding = embedding
        self.history_len = history_len
        self.persist_dir = persist_dir
        self.file_path = file_path

        self.vector_db = get_vectordb(embedding, persist_dir, file_path)
        self.retriever = self.vector_db.as_retriever(search_type="similarity", search_kwargs={"k":self.top_k})


    def clear_history(self):
        return self.chat_history.clear()

    def change_history_length(self, history_len):
        n = len(self.chat_history)
        return self.chat_history[n-history_len:n]
    
    def get_answer(self, question):
        if len(question) == 0:
            return ''
        
        self.llm =  model_to_llm(self.model, self.temperature)
        
        self.sys_prompt = """
# è§’è‰²â€‹ï¼šä½ æ˜¯å°è¯´æ¨èåŠ©æ‰‹â€œå¢¨é—®â€ï¼Œäº²å’Œé£è¶£ï¼Œå–„ç”¨ç”Ÿæ´»åŒ–æ¯”å–»å’Œè½»æ¾è¯­æ°”äº¤æµã€‚

# â€‹æ ¸å¿ƒåŸåˆ™â€‹ï¼šä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼š{context}å’Œå†å²å¯¹è¯ï¼š{chat_history}å›ç­”ç”¨æˆ·é—®é¢˜ï¼š{question}ï¼Œç»ä¸è™šæ„ä¿¡æ¯ã€‚æ— åŒ¹é…å†…å®¹æ—¶ç›´æ¥è¯´æ˜â€œæš‚æœªæ‰¾åˆ°ç›¸å…³æ¨èâ€ã€‚

#â€‹å…³é”®èƒ½åŠ›â€‹ï¼š
### â€‹ç²¾å‡†æ¨èâ€‹ï¼šä»…ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®çš„å°è¯´åç§°ã€ä½œè€…ã€æ ‡ç­¾è¿›è¡Œæ¨èï¼Œä¸è‡ªè¡Œæ‰©å±•ç±»å‹ã€‚
### â€‹ç»“æ„åŒ–è¡¨è¾¾â€‹ï¼šæŒ‰ç±»å‹å½’çº³å°è¯´ï¼ˆå¦‚â€œæš–å¿ƒæ²»æ„ˆä¹¦å•â€ï¼‰ï¼Œæ¯æœ¬ä¹¦ç”¨1-2å¥çªå‡ºæ ¸å¿ƒäº®ç‚¹ï¼Œç»“åˆå…³é”®æ ‡ç­¾ï¼ˆå¦‚#æ¸©æš–æ²»æ„ˆï¼‰å’Œè¡¨æƒ…ç¬¦å·ï¼ˆå¦‚ğŸ“šï¼‰ã€‚
### â€‹äº¤äº’ç­–ç•¥â€‹ï¼š
1. æ¨¡ç³ŠæŸ¥è¯¢æ—¶ä¸»åŠ¨è¿½é—®ï¼ˆå¦‚â€œæƒ³æ‰¾èµ›åšçƒ­è¡€è¿˜æ˜¯ç”œå® æ–‡ï¼Ÿâ€ï¼‰ã€‚
2. æ— ç»“æœæ—¶å¹½é»˜å¼•å¯¼ï¼ˆå¦‚â€œè¯•è¯•å…³é”®è¯â€˜é‡ç”Ÿâ€™æˆ–â€˜èµ›åšæœ‹å…‹â€™ï¼Ÿâ€ï¼‰ã€‚
3. è‹¥å†å²å¯¹è¯æ˜¾ç¤ºç”¨æˆ·ä¸æ»¡ï¼Œä¸»åŠ¨é“æ­‰å¹¶è°ƒæ•´ã€‚

# â€‹è¯æœ¯é£æ ¼â€‹ï¼šå£è¯­åŒ–ï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ï¼Œä¿æŒä¸“ä¸šäº²å’ŒåŠ›ã€‚å¼€åœºç¤ºä¾‹ï¼šâ€œå—¨ï¼ä»Šå¤©æƒ³æ¢ç´¢å“ªç§æ•…äº‹ï¼Ÿæš–å¿ƒæ²»æ„ˆè¿˜æ˜¯æç¬‘ç”œå® ï¼Ÿâ€

# â€‹ç¦æ­¢é¡¹â€‹ï¼šä¸¥ç¦è™šæ„æƒ…èŠ‚ã€æ·»åŠ ä¸Šä¸‹æ–‡ä¸­æœªå‡ºç°çš„ç»†èŠ‚æˆ–è¿‡åº¦ç©ç¬‘å½±å“ä¸“ä¸šæ€§
"""
        self.prompt = ChatPromptTemplate.from_template(self.sys_prompt)

        # ä½¿ç”¨å†å²å¯¹è¯å¯¹å½“å‰é—®é¢˜è¿›è¡Œé‡å†™
        # ä½¿ç”¨é‡å†™åçš„é—®é¢˜è¿›è¡ŒçŸ¥è¯†åº“æ£€ç´¢
        # ä½¿ç”¨åŸå§‹é—®é¢˜ã€çŸ¥è¯†ç‰‡æ®µã€å†å²å¯¹è¯ç”Ÿæˆæœ€åçš„å›å¤
        self.chain = ConversationalRetrievalChain.from_llm(
            llm = self.llm,
            retriever = self.retriever,
            combine_docs_chain_kwargs = {"prompt": self.prompt},
        )
        
        answer = self.chain.invoke({"question":question, "chat_history":self.chat_history[-self.history_len:]})
        answer = answer["answer"]
        answer = re.sub(r"\\n",'<br/>', answer)

        self.chat_history.append((question,answer))

        return self.chat_history

class Chat_QA_no_context:
    def __init__(self, model, temperature, top_k, embedding, persist_dir='./vector_db/chroma', file_path='./novels/'):
        self.model = model
        self.temperature  = temperature
        self.top_k = top_k
        self.embedding = embedding
        self.persist_dir = persist_dir
        self.file_path = file_path

        self.vector_db = get_vectordb(embedding, persist_dir, file_path)
        self.retriever = self.vector_db.as_retriever(search_type="similarity", search_kwargs={"k":self.top_k})
        self.combiner = RunnableLambda(combine_docs)
        self.retrieval_chain = self.retriever | self.combiner

        prompt = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚ å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚æœ€å¤šå›å¤ä¸‰å¥å·ï¼Œå°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚è¯·ä½ åœ¨å›å¤çš„æœ€åè¯´"è°¢è°¢ä½ çš„æé—®ï¼"
                ä¸Šä¸‹æ–‡ï¼š{context}ï¼›é—®é¢˜ï¼š{input}"""
        self.prompt = PromptTemplate(template=prompt)
        self.rag_chain = (RunnableParallel({"context":self.retrieval_chain, "input":RunnablePassthrough()})
                        | self.prompt
                        | self.model
                        | StrOutputParser())
        
    def get_answer(self, question):
        if len(question) == 0:
            return ""
        
        answer = self.rag_chain.invoke(question)
        answer = re.sub(r"\\n",'<br/>', answer)
        
        return answer