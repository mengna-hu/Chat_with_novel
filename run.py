import gradio as gr
from QA_chain import Chat_QA_no_context,Chat_QA_with_context
from dotenv import load_dotenv,find_dotenv
from css import design_css

_ = load_dotenv(find_dotenv())

LLM_DICT = {
    "è®¯é£æ˜Ÿç« MAX":"Spark-max",
    "è®¯é£æ˜Ÿç« Ultra":"Spark-Ultra",
    "æ™ºè°±":"Chatglm-4.5",
    "é€šä¹‰åƒé—®":"Qwen-plus",
    "æ–‡å¿ƒä¸€è¨€":"ernie-speed"
}
LLM_LIST = list(LLM_DICT.keys()) # åŒ…å«æ‰€æœ‰æ¨¡å‹çš„æ‰å¹³åˆ—è¡¨
INIT_LLM = "æ–‡å¿ƒä¸€è¨€"

EMBEDDING_LIST = ["è®¯é£æ˜Ÿç«"]
INIT_EMBEDDING = "è®¯é£æ˜Ÿç«"

DEFAULT_DB_PATH = "./novels"
DEFAULT_PERSIST_PATH = "./vector_db/chroma"

def get_model_by_platform(platform):
    return LLM_DICT.get(platform, "")

class Model_center():
    def __init__(self):
        self.chat_with_context = {}
        self.chat_no_context = {}
    
    def chat_with_context_answer(self, question, chat_history, model="generalv3.5", embedding="è®¯é£æ˜Ÿç«", temperature=0.01, top_k=4, history_len=10, file_path=DEFAULT_DB_PATH, persist_dir=DEFAULT_PERSIST_PATH):
        if question==None or len(question) < 1:
            return "",chat_history
        
        try:
            if (model, embedding) not in self.chat_with_context:
                if len(chat_history) > 0:
                    chat_history = [tuple(turn) for turn in chat_history]
                self.chat_with_context[(model, embedding)] = Chat_QA_with_context(LLM_DICT[model],temperature,top_k,chat_history,embedding, history_len, persist_dir,file_path)
            chain = self.chat_with_context[(model, embedding)]
            chat_history = chain.get_answer(question)
            return  "",chat_history
        
        except Exception as e:
            chat_history.append((question,e))
            return "",chat_history
    
    def clear_history(self):
        if len(self.chat_with_context) > 0:
            for chain in self.chat_with_context.values():
                chain.clear_history()


def format_chat_prompt(message, chat_history):
    prompt = ""
    for turn in chat_history:
        user_msg, bot_msg = turn
        prompt = f"{prompt}\nUser:{user_msg}\nAssistant:{bot_msg}"
    prompt = f"{prompt}\nUser:{message}]\nAssistant:"

    return prompt

def update_text_based_on_llm(llm):
    if llm == "è®¯é£æ˜Ÿç« MAX":
        return "Spark-maxï¼Œæ——èˆ°çº§å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡åƒäº¿çº§å‚æ•°è§„æ¨¡ï¼Œæ“…é•¿å¤„ç†å¤æ‚æ¨ç†é“¾å’Œè·¨é¢†åŸŸçŸ¥è¯†èåˆä»»åŠ¡"
    if llm == "è®¯é£æ˜Ÿç« Ultra":
        return "Spark-Ultraï¼Œå…¨é¢å¯¹æ ‡GPT-4 Turboçš„é€šç”¨å‹å¤§è¯­è¨€æ¨¡å‹ï¼Œç»¼åˆæ€§èƒ½å‡è¡¡ä¸”å¼ºå¤§"
    if llm == "æ™ºè°±":
        return "Chatglm-4.5ï¼Œèšç„¦é€šç”¨å¯¹è¯ã€é€»è¾‘æ¨ç†ä¸é•¿æ–‡æœ¬ç”Ÿæˆï¼Œé€‚åˆé«˜ç²¾åº¦çš„çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡"
    if llm == "é€šä¹‰åƒé—®":
        return "Qwen-plusï¼Œä¸“æ³¨äºå¼ºåŒ–æ¨ç†ä»»åŠ¡ï¼Œåœ¨æ•°å­¦æ¨ç†ã€ç¼–ç¨‹ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²"
    if llm == "æ–‡å¿ƒä¸€è¨€":
        return "ernie-speed-128kï¼Œä¾§é‡å“åº”é€Ÿåº¦ï¼Œä¸­æ–‡ç†è§£æ·±åº¦é«˜ï¼Œå“åº”é€Ÿåº¦å¿«â€‹"
    

model_center = Model_center()

custom_css = design_css()
block = gr.Blocks(css=custom_css)

with block as demo:
    with gr.Row(equal_height=True):
        gr.HTML("""
            <div class="title">ğŸ“š å¢¨é—®---å°è¯´æ™ºèƒ½æŸ¥è¯¢åŠ©æ‰‹</div>
            <div class="subtitle">åŸºäºRAG LLMæŠ€æœ¯çš„æ™ºèƒ½å°è¯´æ£€ç´¢ä¸æ¨èç³»ç»Ÿ</div>
            """)
        
    with gr.Row(elem_classes="main-content"):
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(height=500, show_copy_button=True, show_share_button=True, elem_id="transparent-chatbot")
            msg = gr.Textbox(label='æ‚¨å¥½ï¼æƒ³æŸ¥è¯¢ä»€ä¹ˆå°è¯´å‘¢ï¼Ÿ', elem_id="input")
            with gr.Row():
                clear = gr.ClearButton(components=[chatbot], value="æ¸…é™¤å†å²å¯¹è¯", elem_id="clear")

        with gr.Column(scale=1):
            with gr.Accordion("å‚æ•°é…ç½®", open=True, elem_classes="accordion-container") as model_argument:
                temperature = gr.Slider(0,1,value=0.1,step=0.1,label="å›ç­”éšæœºæ€§", interactive=True)
                top_k = gr.Slider(1,10,value=4,step=1,label="æ£€ç´¢è¿”å›ç‰‡æ®µæ•°é‡",interactive=True)
                history_len = gr.Slider(0,100,value=10,step=10,label="ä¿ç•™çš„å¯¹è¯é•¿åº¦",interactive=True)
                
            with gr.Accordion("å¤§æ¨¡å‹é…ç½®", open=True) as model_select:
                llm = gr.Dropdown(
                    LLM_LIST,
                    label="å¤§æ¨¡å‹",
                    value=INIT_LLM,
                    interactive=True
                )
                llm_text = gr.Markdown(
                    value=update_text_based_on_llm(INIT_LLM),
                    label="æ¨¡å‹è¯´æ˜",
                    elem_classes='subtitle'
                )   

            with gr.Accordion("åµŒå…¥æ¨¡å‹é…ç½®", open=True) as embedding_select:
                embeddings = gr.Dropdown(
                    EMBEDDING_LIST, 
                    label="åµŒå…¥æ¨¡å‹", 
                    value=INIT_EMBEDDING,
                    interactive=False)
                
                embedding_text = gr.Markdown(
                    value="æ³¨ï¼šæš‚æ— æ³•åˆ‡æ¢åµŒå…¥æ¨¡å‹",
                    label="æ¨¡å‹è¯´æ˜",
                    elem_classes='subtitle'
                )   
        # è®¾ç½®ç‚¹å‡»äº‹ä»¶ï¼Œç‚¹å‡»æ—¶ä¼ å…¥å¯¹åº”çš„å‡½æ•°
        msg.submit(model_center.chat_with_context_answer, inputs=[msg,chatbot,llm,embeddings,temperature,top_k,history_len],outputs=[msg,chatbot])
        clear.click(model_center.clear_history)
        llm.change(
            fn=update_text_based_on_llm,
            inputs=llm,
            outputs=llm_text
        )

    gr.Markdown("""
        æé†’ï¼šå›ç­”ç”±AIç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ
        """)


gr.close_all()
demo.launch(allowed_paths=['./'])
